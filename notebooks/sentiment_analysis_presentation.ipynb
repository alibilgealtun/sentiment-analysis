{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä NLP Sentiment Analysis for Technician Feedback Classification\n",
    "\n",
    "## A Comprehensive Machine Learning Approach\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Sentiment Analysis Team  \n",
    "**Date:** 2024  \n",
    "**Presentation Duration:** 40-50 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Table of Contents\n",
    "\n",
    "1. **Introduction & Problem Definition** (5 min)\n",
    "2. **Data Loading & Exploration** (8 min)\n",
    "3. **Data Preprocessing** (10 min)\n",
    "4. **Model Building & Training** (12 min)\n",
    "5. **Model Evaluation & Results** (10 min)\n",
    "6. **Conclusion & Future Work** (5 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 1: Introduction & Problem Definition\n",
    "\n",
    "## üéØ What is Sentiment Analysis?\n",
    "\n",
    "**Sentiment Analysis** (also known as opinion mining) is a Natural Language Processing (NLP) technique that identifies and extracts subjective information from text.\n",
    "\n",
    "### Key Concepts:\n",
    "- **Sentiment**: The emotional tone behind words (positive, negative, neutral)\n",
    "- **Classification**: Categorizing text into predefined sentiment classes\n",
    "- **Feature Extraction**: Converting text to numerical representations\n",
    "\n",
    "## üîß Why Technician Feedback Matters?\n",
    "\n",
    "1. **Quality Improvement**: Identify equipment issues early\n",
    "2. **Safety Monitoring**: Detect safety concerns in feedback\n",
    "3. **Resource Allocation**: Optimize training and tool investments\n",
    "4. **Employee Satisfaction**: Track morale and workload issues\n",
    "5. **Predictive Maintenance**: Anticipate equipment failures\n",
    "\n",
    "## üíº Business Use Cases\n",
    "\n",
    "- **Manufacturing**: Monitor production line feedback\n",
    "- **Field Service**: Analyze technician reports\n",
    "- **IT Support**: Classify support ticket sentiment\n",
    "- **Quality Assurance**: Track quality-related feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 2: Data Loading & Exploration\n",
    "\n",
    "## üì¶ Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from collections import Counter\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_curve, auc\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize, LabelEncoder\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÇ Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the technician feedback dataset\n",
    "df = pd.read_csv('../data/technician_feedback.csv')\n",
    "\n",
    "print(f\"üìä Dataset Shape: {df.shape}\")\n",
    "print(f\"üìù Total Samples: {len(df)}\")\n",
    "print(f\"üìã Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"\\nüîç First 10 Records:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "print(\"\\nüìã Dataset Info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"üìà Statistical Summary:\")\n",
    "print(\"\\n--- Sentiment Distribution ---\")\n",
    "print(df['sentiment'].value_counts())\n",
    "print(\"\\n--- Category Distribution ---\")\n",
    "print(df['category'].value_counts())\n",
    "\n",
    "# Text length statistics\n",
    "df['text_length'] = df['feedback_text'].apply(len)\n",
    "df['word_count'] = df['feedback_text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "print(\"\\n--- Text Statistics ---\")\n",
    "print(df[['text_length', 'word_count']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Class Distribution Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colors for sentiments\n",
    "colors = {'positive': '#2ecc71', 'negative': '#e74c3c', 'neutral': '#3498db'}\n",
    "\n",
    "# Create figure with two subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar Chart\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "ax1 = axes[0]\n",
    "bars = ax1.bar(sentiment_counts.index, sentiment_counts.values, \n",
    "               color=[colors[s] for s in sentiment_counts.index])\n",
    "ax1.set_xlabel('Sentiment')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('Sentiment Distribution (Bar Chart)', fontweight='bold')\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "             f'{int(height)}', ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "# Pie Chart\n",
    "ax2 = axes[1]\n",
    "ax2.pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%',\n",
    "        colors=[colors[s] for s in sentiment_counts.index], explode=[0.02]*3,\n",
    "        shadow=True, startangle=90)\n",
    "ax2.set_title('Sentiment Distribution (Pie Chart)', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/sentiment_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"üìä Sentiment distribution visualized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category distribution\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "category_counts = df['category'].value_counts()\n",
    "bars = ax.barh(category_counts.index, category_counts.values, color='#3498db')\n",
    "ax.set_xlabel('Count')\n",
    "ax.set_ylabel('Category')\n",
    "ax.set_title('Feedback by Category', fontweight='bold')\n",
    "\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    ax.text(width + 1, bar.get_y() + bar.get_height()/2.,\n",
    "            f'{int(width)}', ha='left', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/category_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Word Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def get_word_freq(texts):\n",
    "    \"\"\"Get word frequencies from texts.\"\"\"\n",
    "    all_words = []\n",
    "    for text in texts:\n",
    "        # Tokenize and clean\n",
    "        words = word_tokenize(str(text).lower())\n",
    "        words = [w for w in words if w.isalpha() and w not in stop_words and len(w) > 2]\n",
    "        all_words.extend(words)\n",
    "    return Counter(all_words)\n",
    "\n",
    "# Word frequency by sentiment\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, sentiment in enumerate(['positive', 'negative', 'neutral']):\n",
    "    texts = df[df['sentiment'] == sentiment]['feedback_text']\n",
    "    word_freq = get_word_freq(texts)\n",
    "    top_words = word_freq.most_common(15)\n",
    "    \n",
    "    words, counts = zip(*top_words)\n",
    "    axes[i].barh(words, counts, color=colors[sentiment])\n",
    "    axes[i].set_xlabel('Frequency')\n",
    "    axes[i].set_title(f'Top Words - {sentiment.title()}', fontweight='bold')\n",
    "    axes[i].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/word_frequency.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚òÅÔ∏è Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word clouds for each sentiment\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "colormaps = {'positive': 'Greens', 'negative': 'Reds', 'neutral': 'Blues'}\n",
    "\n",
    "for i, sentiment in enumerate(['positive', 'negative', 'neutral']):\n",
    "    texts = df[df['sentiment'] == sentiment]['feedback_text']\n",
    "    combined_text = ' '.join(texts.values)\n",
    "    \n",
    "    wordcloud = WordCloud(\n",
    "        width=800, height=400,\n",
    "        background_color='white',\n",
    "        colormap=colormaps[sentiment],\n",
    "        max_words=100,\n",
    "        stopwords=stop_words\n",
    "    ).generate(combined_text)\n",
    "    \n",
    "    axes[i].imshow(wordcloud, interpolation='bilinear')\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f'{sentiment.title()} Sentiment', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/wordclouds.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"‚òÅÔ∏è Word clouds generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 3: Data Preprocessing\n",
    "\n",
    "## üîß Text Preprocessing Steps\n",
    "\n",
    "1. **Lowercasing** - Convert all text to lowercase\n",
    "2. **Punctuation Removal** - Remove special characters\n",
    "3. **Tokenization** - Split text into words\n",
    "4. **Stopword Removal** - Remove common words\n",
    "5. **Lemmatization** - Reduce words to base form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"Text preprocessing class for NLP tasks.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and preprocess text.\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "        \n",
    "        # Remove numbers\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        # Remove punctuation\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def tokenize_and_lemmatize(self, text):\n",
    "        \"\"\"Tokenize, remove stopwords, and lemmatize.\"\"\"\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [self.lemmatizer.lemmatize(t) for t in tokens \n",
    "                  if t not in self.stop_words and len(t) > 2]\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Full preprocessing pipeline.\"\"\"\n",
    "        cleaned = self.clean_text(text)\n",
    "        processed = self.tokenize_and_lemmatize(cleaned)\n",
    "        return processed\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "print(\"‚úÖ TextPreprocessor initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Before/After Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate preprocessing\n",
    "print(\"\\nüìã Preprocessing Examples:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sample_texts = df['feedback_text'].head(5).values\n",
    "\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "    processed = preprocessor.preprocess(text)\n",
    "    print(f\"Example {i}:\")\n",
    "    print(f\"  BEFORE: {text}\")\n",
    "    print(f\"  AFTER:  {processed}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to all texts\n",
    "print(\"\\nüîÑ Preprocessing all texts...\")\n",
    "df['processed_text'] = df['feedback_text'].apply(preprocessor.preprocess)\n",
    "print(f\"‚úÖ Processed {len(df)} texts\")\n",
    "\n",
    "# Show sample\n",
    "df[['feedback_text', 'processed_text', 'sentiment']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¢ Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = df['processed_text'].values\n",
    "y = df['sentiment'].values\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"üìä Training set: {len(X_train)} samples\")\n",
    "print(f\"üìä Test set: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization\n",
    "print(\"\\nüìä TF-IDF Vectorization:\")\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"  Training features shape: {X_train_tfidf.shape}\")\n",
    "print(f\"  Test features shape: {X_test_tfidf.shape}\")\n",
    "print(f\"  Vocabulary size: {len(tfidf_vectorizer.vocabulary_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vectorization\n",
    "print(\"\\nüìä Count Vectorization:\")\n",
    "\n",
    "count_vectorizer = CountVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "X_test_count = count_vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"  Training features shape: {X_train_count.shape}\")\n",
    "print(f\"  Test features shape: {X_test_count.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 4: Model Building & Training\n",
    "\n",
    "## ü§ñ Models to Train\n",
    "\n",
    "1. **Naive Bayes** - Probabilistic classifier\n",
    "2. **Support Vector Machine (SVM)** - Hyperplane-based classifier\n",
    "3. **Logistic Regression** - Linear classifier\n",
    "4. **Random Forest** - Ensemble method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "results = {}\n",
    "trained_models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîµ NAIVE BAYES CLASSIFIER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train model\n",
    "nb_model = MultinomialNB(alpha=1.0)\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_nb = nb_model.predict(X_test_tfidf)\n",
    "y_proba_nb = nb_model.predict_proba(X_test_tfidf)\n",
    "\n",
    "# Calculate metrics\n",
    "results['Naive Bayes'] = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_nb),\n",
    "    'precision': precision_score(y_test, y_pred_nb, average='weighted'),\n",
    "    'recall': recall_score(y_test, y_pred_nb, average='weighted'),\n",
    "    'f1_score': f1_score(y_test, y_pred_nb, average='weighted')\n",
    "}\n",
    "trained_models['Naive Bayes'] = (nb_model, y_pred_nb, y_proba_nb)\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "for metric, value in results['Naive Bayes'].items():\n",
    "    print(f\"  {metric.title()}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üü¢ SUPPORT VECTOR MACHINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train model\n",
    "svm_model = SVC(kernel='linear', C=1.0, probability=True, random_state=42)\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_svm = svm_model.predict(X_test_tfidf)\n",
    "y_proba_svm = svm_model.predict_proba(X_test_tfidf)\n",
    "\n",
    "# Calculate metrics\n",
    "results['SVM'] = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_svm),\n",
    "    'precision': precision_score(y_test, y_pred_svm, average='weighted'),\n",
    "    'recall': recall_score(y_test, y_pred_svm, average='weighted'),\n",
    "    'f1_score': f1_score(y_test, y_pred_svm, average='weighted')\n",
    "}\n",
    "trained_models['SVM'] = (svm_model, y_pred_svm, y_proba_svm)\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "for metric, value in results['SVM'].items():\n",
    "    print(f\"  {metric.title()}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üü° LOGISTIC REGRESSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train model\n",
    "lr_model = LogisticRegression(C=1.0, max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_lr = lr_model.predict(X_test_tfidf)\n",
    "y_proba_lr = lr_model.predict_proba(X_test_tfidf)\n",
    "\n",
    "# Calculate metrics\n",
    "results['Logistic Regression'] = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_lr),\n",
    "    'precision': precision_score(y_test, y_pred_lr, average='weighted'),\n",
    "    'recall': recall_score(y_test, y_pred_lr, average='weighted'),\n",
    "    'f1_score': f1_score(y_test, y_pred_lr, average='weighted')\n",
    "}\n",
    "trained_models['Logistic Regression'] = (lr_model, y_pred_lr, y_proba_lr)\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "for metric, value in results['Logistic Regression'].items():\n",
    "    print(f\"  {metric.title()}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4Ô∏è‚É£ Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üü† RANDOM FOREST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_rf = rf_model.predict(X_test_tfidf)\n",
    "y_proba_rf = rf_model.predict_proba(X_test_tfidf)\n",
    "\n",
    "# Calculate metrics\n",
    "results['Random Forest'] = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_rf),\n",
    "    'precision': precision_score(y_test, y_pred_rf, average='weighted'),\n",
    "    'recall': recall_score(y_test, y_pred_rf, average='weighted'),\n",
    "    'f1_score': f1_score(y_test, y_pred_rf, average='weighted')\n",
    "}\n",
    "trained_models['Random Forest'] = (rf_model, y_pred_rf, y_proba_rf)\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "for metric, value in results['Random Forest'].items():\n",
    "    print(f\"  {metric.title()}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîß Hyperparameter Tuning (GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîß HYPERPARAMETER TUNING - Logistic Regression\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1.0, 10.0]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "grid_search = GridSearchCV(\n",
    "    LogisticRegression(max_iter=1000, random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1_weighted',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(f\"\\nBest Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV Score: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Cross-Validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä CROSS-VALIDATION RESULTS (5-Fold)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cv_results = {}\n",
    "models_for_cv = {\n",
    "    'Naive Bayes': MultinomialNB(alpha=1.0),\n",
    "    'SVM': SVC(kernel='linear', C=1.0, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(C=1.0, max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "for name, model in models_for_cv.items():\n",
    "    scores = cross_val_score(model, X_train_tfidf, y_train, cv=5, scoring='f1_weighted')\n",
    "    cv_results[name] = {'mean': scores.mean(), 'std': scores.std()}\n",
    "    print(f\"{name}: {scores.mean():.4f} (+/- {scores.std()*2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 5: Model Evaluation & Results\n",
    "\n",
    "## üìà Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df = results_df.round(4)\n",
    "\n",
    "print(\"\\nüìä Model Performance Comparison:\")\n",
    "print(\"=\"*70)\n",
    "print(results_df.to_string())\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Highlight best model\n",
    "best_model = results_df['f1_score'].idxmax()\n",
    "best_score = results_df.loc[best_model, 'f1_score']\n",
    "print(f\"\\nüèÜ Best Model: {best_model} (F1-Score: {best_score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Model Comparison Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison bar chart\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "x = np.arange(len(results))\n",
    "width = 0.2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "colors_bar = plt.cm.Set2(np.linspace(0, 1, 4))\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    values = [results[model][metric] for model in results]\n",
    "    offset = (i - len(metrics)/2 + 0.5) * width\n",
    "    bars = ax.bar(x + offset, values, width, label=metric.replace('_', ' ').title(), color=colors_bar[i])\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars, values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{value:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "ax.set_xlabel('Model', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(results.keys())\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_ylim([0, 1.15])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî• Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "classes = sorted(np.unique(y_test))\n",
    "\n",
    "for idx, (name, (model, y_pred, y_proba)) in enumerate(trained_models.items()):\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=classes)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=classes, yticklabels=classes, ax=axes[idx])\n",
    "    axes[idx].set_xlabel('Predicted')\n",
    "    axes[idx].set_ylabel('Actual')\n",
    "    axes[idx].set_title(f'{name}\\nAccuracy: {results[name][\"accuracy\"]:.4f}', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "classes = sorted(np.unique(y_test))\n",
    "y_test_bin = label_binarize(y_test, classes=classes)\n",
    "\n",
    "for idx, (name, (model, y_pred, y_proba)) in enumerate(trained_models.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    for i, class_name in enumerate(classes):\n",
    "        fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_proba[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        ax.plot(fpr, tpr, lw=2, label=f'{class_name} (AUC = {roc_auc:.3f})')\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title(f'{name} - ROC Curves', fontweight='bold')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Feature Importance (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from Random Forest\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# Get top 20 features\n",
    "top_idx = np.argsort(importances)[-20:][::-1]\n",
    "top_features = [(feature_names[i], importances[i]) for i in top_idx]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "features, scores = zip(*top_features)\n",
    "y_pos = np.arange(len(features))\n",
    "\n",
    "ax.barh(y_pos, scores, color='#3498db', alpha=0.8)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(features)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Importance Score')\n",
    "ax.set_title('Top 20 Feature Importance (Random Forest)', fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 6: Conclusion\n",
    "\n",
    "## üéØ Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã KEY FINDINGS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£  Dataset Analysis:\")\n",
    "print(f\"    - Total samples: {len(df)}\")\n",
    "print(f\"    - Sentiment distribution: Positive ({df['sentiment'].value_counts()['positive']}), \"\n",
    "      f\"Negative ({df['sentiment'].value_counts()['negative']}), \"\n",
    "      f\"Neutral ({df['sentiment'].value_counts()['neutral']})\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£  Model Performance:\")\n",
    "for model, metrics in sorted(results.items(), key=lambda x: x[1]['f1_score'], reverse=True):\n",
    "    print(f\"    - {model}: F1={metrics['f1_score']:.4f}, Accuracy={metrics['accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£  Best Model: {best_model}\")\n",
    "print(f\"    - F1-Score: {best_score:.4f}\")\n",
    "print(f\"    - Accuracy: {results[best_model]['accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£  Feature Engineering:\")\n",
    "print(f\"    - TF-IDF features: {X_train_tfidf.shape[1]}\")\n",
    "print(f\"    - N-gram range: (1, 2)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÆ Future Improvements\n",
    "\n",
    "1. **Deep Learning Models**\n",
    "   - LSTM Neural Networks\n",
    "   - BERT-based models (DistilBERT)\n",
    "\n",
    "2. **Enhanced Features**\n",
    "   - Word embeddings (Word2Vec, GloVe)\n",
    "   - Sentiment lexicons\n",
    "\n",
    "3. **Model Enhancements**\n",
    "   - Ensemble methods\n",
    "   - Model explainability (LIME, SHAP)\n",
    "\n",
    "4. **Production Deployment**\n",
    "   - API endpoint\n",
    "   - Real-time predictions\n",
    "\n",
    "## üôè Thank You!\n",
    "\n",
    "**Questions?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models for later use\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "os.makedirs('../outputs', exist_ok=True)\n",
    "\n",
    "# Save vectorizer\n",
    "joblib.dump(tfidf_vectorizer, '../models/tfidf_vectorizer.joblib')\n",
    "\n",
    "# Save models\n",
    "for name, (model, _, _) in trained_models.items():\n",
    "    filename = name.lower().replace(' ', '_')\n",
    "    model_data = {\n",
    "        'model': model,\n",
    "        'label_encoder': LabelEncoder().fit(y),\n",
    "        'classes_': sorted(np.unique(y)),\n",
    "        'is_fitted': True\n",
    "    }\n",
    "    joblib.dump(model_data, f'../models/{filename}_model.joblib')\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv('../models/model_results.csv')\n",
    "\n",
    "print(\"\\n‚úÖ All models and results saved!\")\n",
    "print(\"\\nSaved files:\")\n",
    "for f in os.listdir('../models'):\n",
    "    print(f\"  - models/{f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
